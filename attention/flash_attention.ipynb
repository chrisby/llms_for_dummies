{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8eab68a-2679-45fc-919b-3b0467b74f76",
   "metadata": {},
   "source": [
    "# FlashAttention: Solving the Memory Bottleneck\n",
    "\n",
    "In the [previous notebook](./standard_attention.ipynb), we explored why attention becomes a memory bottleneck:\n",
    "\n",
    "- **No tiling / 1D tiling**: O(N) HBM traffic, but can't scale — the N×N attention matrix (or K+V) overflows SRAM\n",
    "- **2D tiling**: Scales to any N, but has O(N²) HBM traffic because softmax needs full rows, forcing us to write/read the N×N matrices S and P through HBM\n",
    "\n",
    "**The question**: Can we do 2D tiling without materializing S and P?\n",
    "\n",
    "**FlashAttention's insight**: We don't actually need the full row of S to compute softmax! We can compute it *incrementally* using **online softmax** — maintaining running statistics that let us correct partial results as we see more data. This eliminates the S and P round-trips, significantly reducing HBM traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24qzgwk0ft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Simulated A100\n",
      "SRAM capacity: 131,072 floats (256 KB)\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import GPU simulator and 2D tiled attention for comparison\n",
    "from llms_for_dummies.gpu_sim import GPUSpec, Profiler, Tensor\n",
    "import math\n",
    "\n",
    "gpu = GPUSpec.sim_a100()\n",
    "Tensor.verbose = False  # Less noise for this notebook\n",
    "\n",
    "print(f\"GPU: {gpu.name}\")\n",
    "print(f\"SRAM capacity: {gpu.sram_size:,} floats ({gpu.sram_size * gpu.bytes_per_float / 1024:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3caaec2-c5af-4caf-b143-0e2e086b3176",
   "metadata": {},
   "source": [
    "## 1. The Online Softmax Trick\n",
    "\n",
    "Standard softmax requires the **global max** and **global sum** before generating any output:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\textbf{x})_i = \\frac{e^{\\textbf{x}_i - m}}{l}\n",
    "$$\n",
    "\n",
    "Where $m = \\max(\\textbf{x})$ and $l = \\sum_j e^{\\textbf{x}_j - m}$.\n",
    "\n",
    "**The Problem:** We can't compute $m$ or $l$ without seeing the entire row (all $N$ tokens), which requires huge HBM reads/writes.\n",
    "\n",
    "**The Solution:** Compute softmax **iteratively**. As we load new chunks of $K$ and $V$, we update our running statistics and rescale previous results on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1f572-5b29-4b4f-89a9-36912da834e1",
   "metadata": {},
   "source": [
    "### The Iterative Update Logic\n",
    "\n",
    "Let's say we have processed some previous blocks and have a running state ($m_{old}, l_{old}, O_{old}$). We now load a **new block** of size $B_c$.\n",
    "\n",
    "**1. Compute Local Block Statistics**\n",
    "\n",
    "Calculate the max and unnormalized scores for just the current tile:\n",
    "$$m_{block} = \\max(S_{block})$$\n",
    "$$P_{block} = e^{S_{block} - m_{block}}$$\n",
    "$$l_{block} = \\sum P_{block}$$\n",
    "\n",
    "**2. Update Global Statistics**\n",
    "\n",
    "Compare the new block's max with our running max:\n",
    "$$m_{new} = \\max(m_{old}, m_{block})$$\n",
    "\n",
    "Calculate **rescale factors** to shift everything to the common baseline $m_{new}$:\n",
    "$$\\alpha = e^{m_{old} - m_{new}} \\quad \\text{(Shrink factor for history)}$$\n",
    "$$\\beta = e^{m_{block} - m_{new}} \\quad \\text{(Shrink factor for current block)}$$\n",
    "\n",
    "Update the running sum (denominator):\n",
    "$$l_{new} = \\underbrace{l_{old} \\cdot \\alpha}_{\\text{decayed history}} + \\underbrace{l_{block} \\cdot \\beta}_{\\text{new contribution}}$$\n",
    "\n",
    "**3. Update the Output Accumulator ($O$)**\n",
    "\n",
    "This is the critical step, we accumulate the weighted sum directly into $O$. We must **decay the old accumulator** so it matches the scale of the new block.\n",
    "\n",
    "$$\n",
    "O_{new} = \\underbrace{O_{old} \\cdot \\alpha}_{\\text{Rescale old sums}} + \\underbrace{(P_{block} \\cdot V_{block}) \\beta}_{\\text{Add new weighted contribution}}\n",
    "$$\n",
    "\n",
    "> **Key Insight:** We never materialize the full $P$ (or $S$) matrices. We compute the contribution of a small tile ($P_{block} \\times V_{block}$), add it to our running total, and immediately discard $P_{block}$.\n",
    "\n",
    "**4. Final Normalization**\n",
    "\n",
    "After iterating through all blocks, $O_{final}$ contains the unnormalized weighted sum. One final division gives the correct attention output:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\frac{O_{final}}{l_{final}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xwm10rm24ps",
   "metadata": {},
   "source": [
    "### Worked Example: Online Softmax in Action\n",
    "\n",
    "Let's trace through online softmax with concrete numbers. We'll compute attention for a single query attending to 5 keys, processing them in two blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe776341-8fec-4a80-a84a-5c6ec995c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS:\n",
      "  Scores (S_row): [1. 4. 2. 5. 3.]\n",
      "  Values (V): \n",
      "[[0.1 0.2 0.3]\n",
      " [1.  1.  1. ]\n",
      " [0.5 0.  0.5]\n",
      " [2.  2.  0. ]\n",
      " [0.1 0.8 0.1]]\n",
      "============================================================\n",
      "STEP 1: Processing Block 1 (Tokens 0, 1)\n",
      "  [Stats] m_old=4.0, l_old=1.0498\n",
      "  [Accumulator O_old] [1.00497871 1.00995741 1.01493612]  <-- Vector of size 3\n",
      "------------------------------------------------------------\n",
      "STEP 2: Processing Block 2 (Tokens 2, 3, 4)\n",
      "  [Stats] m_block=5.0, l_block=1.1851\n",
      "  [Block Contrib] [2.03842706 2.10826823 0.03842706]\n",
      "------------------------------------------------------------\n",
      "STEP 3: Merging...\n",
      "  [Rescale] alpha=0.3679, beta=1.0000\n",
      "  [Update]  O_new = ([1.00497871 1.00995741 1.01493612] * 0.37) + ([2.03842706 2.10826823 0.03842706] * 1.00)\n",
      "  [Result]  O_new = [2.40813807 2.4798108  0.4118012 ]\n",
      "============================================================\n",
      "STEP 4: Final Normalization\n",
      "  Final Output Vector: [1.53255989 1.57817303 0.26207384]\n",
      "------------------------------------------------------------\n",
      "  Ground Truth:        [1.53255989 1.57817303 0.26207384]\n",
      "  Match? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We are calculating Attention for a SINGLE Query token (i)\n",
    "# attending to 5 Key/Value tokens.\n",
    "d = 3  # Head dimension (vector size)\n",
    "\n",
    "# 1. Attention Scores (S_i) = Q_i @ K.T\n",
    "#    This is the row of raw dot products before softmax.\n",
    "S_row = np.array([1.0, 4.0, 2.0, 5.0, 3.0])  \n",
    "\n",
    "# 2. Value Matrix (V)\n",
    "#    Each of the 5 tokens has a Value vector of size d=3\n",
    "V = np.array([\n",
    "    [0.1, 0.2, 0.3],  # Token 0\n",
    "    [1.0, 1.0, 1.0],  # Token 1\n",
    "    [0.5, 0.0, 0.5],  # Token 2\n",
    "    [2.0, 2.0, 0.0],  # Token 3\n",
    "    [0.1, 0.8, 0.1],  # Token 4\n",
    "])\n",
    "\n",
    "print(f\"INPUTS:\")\n",
    "print(f\"  Scores (S_row): {S_row}\")\n",
    "print(f\"  Values (V): \\n{V}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# Step 1: Process First Block (The History)\n",
    "# ==========================================\n",
    "# We load the first 2 tokens (Block 1)\n",
    "print(\"STEP 1: Processing Block 1 (Tokens 0, 1)\")\n",
    "S_block = S_row[:2]      # [1.0, 4.0]\n",
    "V_block = V[:2]          # First two rows of V\n",
    "\n",
    "# Local computations\n",
    "m_old = S_block.max()                  # Scalar: 4.0\n",
    "P_block = np.exp(S_block - m_old)      # Vector: [0.05, 1.0]\n",
    "l_old = P_block.sum()                  # Scalar: 1.05\n",
    "\n",
    "# Compute Accumulator (Weighted Sum of V vectors)\n",
    "# Shape: (2,) @ (2, 3) -> (3,)\n",
    "O_old = P_block @ V_block                   \n",
    "\n",
    "print(f\"  [Stats] m_old={m_old}, l_old={l_old:.4f}\")\n",
    "print(f\"  [Accumulator O_old] {O_old}  <-- Vector of size {d}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# Step 2: Process Second Block (The Update)\n",
    "# ==========================================\n",
    "# We load the next 3 tokens (Block 2)\n",
    "print(\"STEP 2: Processing Block 2 (Tokens 2, 3, 4)\")\n",
    "S_block = S_row[2:]      # [2.0, 5.0, 3.0]\n",
    "V_block = V[2:]          # Last three rows of V\n",
    "\n",
    "# Local computations for the new block\n",
    "m_block = S_block.max()                # Scalar: 5.0\n",
    "P_block = np.exp(S_block - m_block)    # Vector: [0.05, 1.0, 0.14]\n",
    "l_block = P_block.sum()                # Scalar: 1.185...\n",
    "\n",
    "# Compute Contribution from this block\n",
    "O_contrib = P_block @ V_block          # Shape (3,)\n",
    "\n",
    "print(f\"  [Stats] m_block={m_block}, l_block={l_block:.4f}\")\n",
    "print(f\"  [Block Contrib] {O_contrib}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# Step 3: The Online Softmax Merge\n",
    "# ==========================================\n",
    "print(\"STEP 3: Merging...\")\n",
    "\n",
    "# 1. Update Global Max\n",
    "m_new = max(m_old, m_block)\n",
    "\n",
    "# 2. Calculate Rescale Factors\n",
    "#    alpha: decays the old history\n",
    "#    beta:  decays the new block (usually 1.0 if new block has higher max)\n",
    "alpha = np.exp(m_old - m_new)  # exp(4-5) = 0.3679\n",
    "beta  = np.exp(m_block - m_new) # exp(5-5) = 1.0\n",
    "\n",
    "# 3. Update Global Sum\n",
    "l_new = (l_old * alpha) + (l_block * beta)\n",
    "\n",
    "# 4. Update Output Accumulator (Vector operation)\n",
    "#    Formula: O_new = (O_old * alpha) + (O_contrib * beta)\n",
    "O_new = (O_old * alpha) + (O_contrib * beta)\n",
    "\n",
    "print(f\"  [Rescale] alpha={alpha:.4f}, beta={beta:.4f}\")\n",
    "print(f\"  [Update]  O_new = ({O_old} * {alpha:.2f}) + ({O_contrib} * {beta:.2f})\")\n",
    "print(f\"  [Result]  O_new = {O_new}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# Step 4: Finalize\n",
    "# ==========================================\n",
    "print(\"STEP 4: Final Normalization\")\n",
    "\n",
    "# Divide vector by scalar sum\n",
    "final_output = O_new / l_new\n",
    "\n",
    "print(f\"  Final Output Vector: {final_output}\")\n",
    "\n",
    "# Verify against Naive Numpy\n",
    "print(\"-\" * 60)\n",
    "full_P = np.exp(S_row - S_row.max())\n",
    "full_softmax = full_P / full_P.sum()\n",
    "true_output = full_softmax @ V\n",
    "print(f\"  Ground Truth:        {true_output}\")\n",
    "print(f\"  Match? {np.allclose(final_output, true_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886115e-d5f0-4d1d-9fef-3e22e7fc8eea",
   "metadata": {},
   "source": [
    "### Why Does Rescaling Work?\n",
    "\n",
    "The key insight is that **softmax is invariant to shifting by a constant**:\n",
    "$$\\text{softmax}(x - c) = \\text{softmax}(x)$$\n",
    "\n",
    "When we see a new chunk with a larger max ($m_{new}$), we need to \"shift\" our previous computations to align with it. The rescaling factor `exp(m_old - m_new)` accounts for this shift.\n",
    "\n",
    "1. **Scaling Individual Terms:**\n",
    "   Each old value is effectively scaled down to make room for the new maximum:\n",
    "\n",
    "```\n",
    "Old: exp(x - m_old)\n",
    "New: exp(x - m_new) = exp(x - m_old) × exp(m_old - m_new)\n",
    "                                        └─── rescale ───┘\n",
    "```\n",
    "\n",
    "2. **Scaling the Sum (`l`):**\n",
    "Since *every* term in the old sum is scaled by the exact same factor, we can factor it out. This allows us to update the running sum without recalculating individual elements:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l_{new} &= \\sum_{i} \\left( \\exp(x_i - m_{old}) \\times \\text{scale} \\right) \\\\\n",
    "&= \\text{scale} \\times \\sum_{i} \\exp(x_i - m_{old}) \\\\\n",
    "&= \\text{scale} \\times l_{old}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ecfe1-53eb-488c-8733-ca24160d026b",
   "metadata": {},
   "source": [
    "## 2. The FlashAttention Algorithm\n",
    "\n",
    "For each row of Q, we maintain three running values as we iterate through K/V tiles:\n",
    "\n",
    "| State | Meaning | Update Rule |\n",
    "|-------|---------|-------------|\n",
    "| `m` | Running max of S values seen so far | `m = max(m, max(S_tile))` |\n",
    "| `l` | Running sum of exp(S - m) | `l = l × α + sum(exp(S_tile - m)) × β` |\n",
    "| `O` | Running weighted sum (unnormalized output) | `O = O × α + (P_tile @ V_tile) × β` |\n",
    "\n",
    "Where:\n",
    "- `α = exp(m_old - m_new)` — rescales old values when max increases\n",
    "- `β = exp(m_tile - m_new)` — rescales new tile values\n",
    "\n",
    "After processing all K/V tiles, we finalize: `O_final = O / l`\n",
    "\n",
    "**The magic**: We never store the full S or P matrices. Each tile is processed and immediately used to update our running state!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffdb05-b1be-4312-84f6-eb1457f9efb8",
   "metadata": {},
   "source": [
    "### FlashAttention Processing Pattern\n",
    "\n",
    "```\n",
    "For each Q tile (outer loop):                        \n",
    "┌─────────────────────────────────────────────┐      \n",
    "│ Q tile    ──┐                               │      \n",
    "│  (Br×d)     │                               │      \n",
    "└─────────────│───────────────────────────────┘      \n",
    "              │                                       \n",
    "              ▼                                       \n",
    "┌─────────┬─────────┬─────────┬─────────┐           \n",
    "│ K tile 1│ K tile 2│ K tile 3│   ...   │ ◄── inner loop\n",
    "│  (Bc×d) │  (Bc×d) │  (Bc×d) │         │           \n",
    "└────┬────┴────┬────┴────┬────┴─────────┘           \n",
    "     │         │         │                           \n",
    "┌────▼────┬────▼────┬────▼────┐                      \n",
    "│ S tile 1│ S tile 2│ S tile 3│  Never written to HBM!\n",
    "│(Br×Bc)  │(Br×Bc)  │(Br×Bc)  │  Immediately:\n",
    "└────┬────┴────┬────┴────┬────┘  1. Update m, l      \n",
    "     │         │         │       2. Compute P tile    \n",
    "     │         │         │       3. Accumulate into O \n",
    "     ▼         ▼         ▼       4. Free from SRAM   \n",
    "┌─────────────────────────────┐                      \n",
    "│      Running State          │                      \n",
    "│  m (Br×1): running max      │                      \n",
    "│  l (Br×1): running sum      │                      \n",
    "│  O (Br×d): running output   │                      \n",
    "└──────────────┬──────────────┘                      \n",
    "               │ After all K tiles                   \n",
    "               ▼                                     \n",
    "┌─────────────────────────────┐                      \n",
    "│  O_final = O / l            │──▶ Write to HBM     \n",
    "└─────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b33ad-11b7-43d1-be38-205bf1707275",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "Now let's implement FlashAttention with our GPU simulator. The code follows the pattern above, handling full matrices with proper tiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966858ca-7be7-47c5-bba2-5dddc2f0769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention(prof: Profiler, N: int, d: int, tile_size: int):\n",
    "    \"\"\"\n",
    "    FlashAttention: 2D tiling with online softmax.\n",
    "    \n",
    "    Key idea: Instead of computing full softmax rows, we:\n",
    "    1. Process K/V in tiles\n",
    "    2. Track running max (m) and sum (l) per row\n",
    "    3. Rescale partial results as we see new tiles\n",
    "    4. Never write S or P to HBM!\n",
    "    \n",
    "    HBM traffic: O(N²d²/M) where M is SRAM size. Still O(N²), but avoids\n",
    "    the S and P round-trips that dominate naive 2D tiling. K and V are\n",
    "    re-read once per Q tile — that's the remaining quadratic cost.\n",
    "    \"\"\"\n",
    "    Q = Tensor((N, d), \"Q\", prof)\n",
    "    K = Tensor((N, d), \"K\", prof)\n",
    "    V = Tensor((N, d), \"V\", prof)\n",
    "    \n",
    "    scale_factor = 1.0 / math.sqrt(d)\n",
    "    \n",
    "    # Outer loop: process Q in row blocks\n",
    "    for i in range(0, N, tile_size):\n",
    "        i_end = min(i + tile_size, N)\n",
    "        num_rows = i_end - i\n",
    "        \n",
    "        q_tile = Q.load(rows=(i, i_end))\n",
    "        \n",
    "        # Online softmax state — these are the running statistics from Section 1:\n",
    "        m = Tensor.zeros((num_rows, 1), \"m\", prof)\n",
    "        l = Tensor.zeros((num_rows, 1), \"l\", prof)\n",
    "        o = Tensor.zeros((num_rows, d), \"O_acc\", prof)\n",
    "        \n",
    "        # Inner loop: iterate through K/V tiles\n",
    "        for j in range(0, N, tile_size):\n",
    "            j_end = min(j + tile_size, N)\n",
    "            \n",
    "            k_tile = K.load(rows=(j, j_end))\n",
    "            v_tile = V.load(rows=(j, j_end))\n",
    "            \n",
    "            # --- Equation: S_block = Q_tile @ K_tile.T / sqrt(d) ---\n",
    "            s_tile = (q_tile @ k_tile.T).scale(scale_factor, \"/√d\")\n",
    "            k_tile.free()\n",
    "            \n",
    "            # --- Equation: m_block = max(S_block) ---\n",
    "            m_tile = s_tile.rowmax()\n",
    "            \n",
    "            # --- Equation: P_block = exp(S_block - m_block) ---\n",
    "            s_shifted = s_tile.sub_rowvec(m_tile)\n",
    "            s_tile.free()\n",
    "            p_tile = s_shifted.exp()\n",
    "            s_shifted.free()\n",
    "            \n",
    "            # --- Equation: l_block = sum(P_block) ---\n",
    "            l_tile = p_tile.rowsum()\n",
    "            \n",
    "            # --- Equation: m_new = max(m_old, m_block) ---\n",
    "            m_new = Tensor.zeros((num_rows, 1), \"m_new\", prof)\n",
    "            m_new.add_(m)\n",
    "            m_new.max_(m_tile)\n",
    "            \n",
    "            # --- Equation: α = exp(m_old - m_new), β = exp(m_block - m_new) ---\n",
    "            m_diff_old = m.sub_rowvec(m_new)\n",
    "            m.free()\n",
    "            alpha = m_diff_old.exp()           # α\n",
    "            m_diff_old.free()\n",
    "            \n",
    "            m_diff_new = m_tile.sub_rowvec(m_new)\n",
    "            m_tile.free()\n",
    "            beta = m_diff_new.exp()            # β\n",
    "            m_diff_new.free()\n",
    "            \n",
    "            # --- Equation: l_new = l_old * α + l_block * β ---\n",
    "            l.mul_(alpha)                      # l_old * α  (in-place)\n",
    "            l_tile.mul_(beta)                  # l_block * β (in-place)\n",
    "            l.add_(l_tile)                     # l_old*α + l_block*β\n",
    "            l_tile.free()\n",
    "            \n",
    "            # --- Equation: O_new = O_old * α + (P_block @ V_block) * β ---\n",
    "            o_local = p_tile @ v_tile          # P_block @ V_block\n",
    "            p_tile.free()\n",
    "            v_tile.free()\n",
    "            \n",
    "            o.mul_(alpha)                      # O_old * α\n",
    "            alpha.free()\n",
    "            \n",
    "            o_local.mul_(beta)                 # (P_block @ V_block) * β\n",
    "            beta.free()\n",
    "            \n",
    "            o.add_(o_local)                    # O_new = O_old*α + (P@V)*β\n",
    "            o_local.free()\n",
    "            \n",
    "            # Shift state for next iteration\n",
    "            m = m_new\n",
    "        \n",
    "        # --- Equation: O_final = O / l_final ---\n",
    "        o_final = o.div_rowvec(l)\n",
    "        o.free()\n",
    "        l.free()\n",
    "        m.free()\n",
    "        \n",
    "        # Only the final output touches HBM — S and P never left SRAM!\n",
    "        o_final.write_hbm()\n",
    "        o_final.free()\n",
    "        q_tile.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0820425-4eab-4032-9d2a-69dd6c10a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=32768, d=128, tile_size=158\n",
      "(Smaller than 2D naive because FlashAttention has more intermediate tensors)\n",
      "\n",
      "============================================================\n",
      "FlashAttention (N=32768)\n",
      "============================================================\n",
      "\n",
      "Memory Usage:\n",
      "  Peak HBM:    24.0 MB\n",
      "  Peak SRAM:  │████████████···│  85%\n",
      "\n",
      "HBM Traffic:\n",
      "  Reads:    1,749,024,768 floats (3336.0 MB)\n",
      "  Writes:       4,194,304 floats (8.0 MB)\n",
      "  Total:    1,753,219,072 floats (3344.0 MB)\n",
      "\n",
      "Time Breakdown:\n",
      "  ┌──────────────────────────────────────────────────┐\n",
      "  │██████████████████████████████████░░░░░░░░░░░░░░░░│\n",
      "  └──────────────────────────────────────────────────┘\n",
      "   Computing (68%)                  Waiting for HBM (32%)\n",
      "\n",
      "→ Compute-bound (GPU is busy computing)\n"
     ]
    }
   ],
   "source": [
    "N, d = 32_768, 128\n",
    "tile_size = gpu.optimal_tile_size_flash(d)\n",
    "\n",
    "print(f\"N={N}, d={d}, tile_size={tile_size}\")\n",
    "print(f\"(Smaller than 2D naive because FlashAttention has more intermediate tensors)\")\n",
    "print()\n",
    "\n",
    "Tensor.verbose = False\n",
    "prof = Profiler(gpu, f\"FlashAttention (N={N})\")\n",
    "flash_attention(prof, N, d, tile_size)\n",
    "prof.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263cda8c-c992-4674-8af3-1650d569dbf1",
   "metadata": {},
   "source": [
    "## 4. Comparison: 2D Tiling vs FlashAttention\n",
    "\n",
    "Let's compare naive 2D tiling (from the previous notebook) against FlashAttention on the same problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbc4b4c-e44f-4858-8505-9ce535183335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Comparison: N=32768, d=128\n",
      "============================================================\n",
      "\n",
      "Metric                           2D Tiled  FlashAttention\n",
      "------------------------------------------------------------\n",
      "Tile size................             217             158\n",
      "HBM Reads................   3,426,746,368   1,749,024,768\n",
      "HBM Writes...............   2,151,677,952       4,194,304\n",
      "Total HBM Traffic........   5,578,424,320   1,753,219,072\n",
      "Peak SRAM (%)............            100%             85%\n",
      "\n",
      "FlashAttention uses 3.2× less HBM traffic!\n"
     ]
    }
   ],
   "source": [
    "# Import the 2D tiled attention from the previous notebook for comparison\n",
    "from ipynb.fs.defs.standard_attention import attention_2d_tiled\n",
    "\n",
    "Tensor.verbose= False\n",
    "\n",
    "N, d = 32_768, 128\n",
    "tile_size_2d = gpu.optimal_tile_size_2d(d)\n",
    "tile_size_flash = gpu.optimal_tile_size_flash(d)\n",
    "\n",
    "# Run both 2D tiled and FlashAttention\n",
    "prof_2d = Profiler(gpu, \"2D Tiled (naive)\")\n",
    "attention_2d_tiled(prof_2d, N, d, tile_size_2d)\n",
    "\n",
    "prof_flash = Profiler(gpu, \"FlashAttention\")\n",
    "flash_attention(prof_flash, N, d, tile_size_flash)\n",
    "\n",
    "# Compare\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Comparison: N={N}, d={d}\")\n",
    "print(f\"{'='*60}\")\n",
    "print()\n",
    "print(f\"{'Metric':<25} {'2D Tiled':>15} {'FlashAttention':>15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Tile size':.<25} {tile_size_2d:>15} {tile_size_flash:>15}\")\n",
    "print(f\"{'HBM Reads':.<25} {prof_2d.total_hbm_reads:>15,} {prof_flash.total_hbm_reads:>15,}\")\n",
    "print(f\"{'HBM Writes':.<25} {prof_2d.total_hbm_writes:>15,} {prof_flash.total_hbm_writes:>15,}\")\n",
    "print(f\"{'Total HBM Traffic':.<25} {prof_2d.total_hbm_reads + prof_2d.total_hbm_writes:>15,} {prof_flash.total_hbm_reads + prof_flash.total_hbm_writes:>15,}\")\n",
    "print(f\"{'Peak SRAM (%)':.<25} {prof_2d.peak_sram_usage / gpu.sram_size * 100:>14.0f}% {prof_flash.peak_sram_usage / gpu.sram_size * 100:>14.0f}%\")\n",
    "print()\n",
    "\n",
    "speedup = (prof_2d.total_hbm_reads + prof_2d.total_hbm_writes) / (prof_flash.total_hbm_reads + prof_flash.total_hbm_writes)\n",
    "print(f\"FlashAttention uses {speedup:.1f}× less HBM traffic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sjy8exiyz2d",
   "metadata": {},
   "source": [
    "### How Does This Scale with Sequence Length?\n",
    "\n",
    "The single comparison above shows a 3.2x difference at N=32,768. But how does the ratio behave as N grows?\n",
    "\n",
    "Both approaches are O(N²) — in both cases, K and V are re-read for each Q tile. The difference is a **constant factor**: naive 2D tiling also reads/writes the N×N matrices S and P through HBM, while FlashAttention avoids those entirely. So we expect a roughly constant ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8tlkvfiy4kl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       N    2D Tiled Traffic       Flash Traffic    Ratio\n",
      "------------------------------------------------------------\n",
      "   1,024              11 MB               4 MB     2.8x\n",
      "   4,096             168 MB              54 MB     3.1x\n",
      "  16,384            2664 MB             840 MB     3.2x\n",
      "  32,768           10640 MB            3344 MB     3.2x\n",
      "  65,536           42496 MB           13312 MB     3.2x\n",
      " 131,072          169856 MB           53184 MB     3.2x\n",
      "\n",
      "The ratio is roughly constant — both are O(N²).\n",
      "The ~3x improvement comes from eliminating the S and P round-trips.\n"
     ]
    }
   ],
   "source": [
    "d = 128\n",
    "print(f\"{'N':>8}  {'2D Tiled Traffic':>18}  {'Flash Traffic':>18}  {'Ratio':>7}\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "for N in [1024, 4096, 16384, 32768, 65536, 131072]:\n",
    "    tile_2d = gpu.optimal_tile_size_2d(d)\n",
    "    tile_flash = gpu.optimal_tile_size_flash(d)\n",
    "\n",
    "    p2d = Profiler(gpu, \"2D\")\n",
    "    attention_2d_tiled(p2d, N, d, tile_2d)\n",
    "    traffic_2d = p2d.total_hbm_reads + p2d.total_hbm_writes\n",
    "\n",
    "    pf = Profiler(gpu, \"Flash\")\n",
    "    flash_attention(pf, N, d, tile_flash)\n",
    "    traffic_flash = pf.total_hbm_reads + pf.total_hbm_writes\n",
    "\n",
    "    ratio = traffic_2d / traffic_flash\n",
    "    print(f\"{N:>8,}  {traffic_2d * 2 / 1024**2:>14.0f} MB  {traffic_flash * 2 / 1024**2:>14.0f} MB  {ratio:>6.1f}x\")\n",
    "\n",
    "print()\n",
    "print(\"The ratio is roughly constant — both are O(N²).\")\n",
    "print(\"The ~3x improvement comes from eliminating the S and P round-trips.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7717b-a9bf-4b56-8125-127a5a755ab5",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "FlashAttention eliminates the dominant cost of naive 2D tiling — the S and P round-trips through HBM — by computing softmax online. S and P are never materialized; each tile is computed in SRAM, used immediately to update the running output, and discarded.\n",
    "\n",
    "**What remains**: K and V are still re-read for each Q tile (N/Br times each), giving O(N²d²/M) total traffic. This is still O(N²), but with a much better constant than naive 2D tiling — roughly 3x less traffic in practice.\n",
    "\n",
    "**Where does the 3x come from?** Naive 2D tiling writes S to HBM, reads it back for softmax, writes P, and reads P back for the final matmul — that's ~4 passes over an N×N matrix. FlashAttention avoids all of these. The remaining cost (re-reading K and V) is shared by both approaches.\n",
    "\n",
    "**Why it matters**: Even a constant factor improvement is huge when you're memory-bound. The profiler shows FlashAttention shifts the bottleneck from memory to compute — meaning the GPU is actually doing useful work instead of waiting for HBM. This is why FlashAttention is used in modern LLM training and inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
