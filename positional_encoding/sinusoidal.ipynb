{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Equivariance\n",
    "This is the key concept that motivates the need for positional encodings. To develop an intuition for the concept, let's first answer whether humans are permutation equivariant.\n",
    "\n",
    "## Are Humans?\n",
    "\n",
    "Let $f(x)$ denote a function that takes a natural language query $x$ and produces a \"human-like\" natural language response $y$. Let's consider $x$ to be the sentence\n",
    "* $x=[\\text{The}, \\text{developer}, \\text{deployed}, \\text{the}, \\text{agent}, \\text{to}, \\text{production}]$\n",
    "\n",
    "A \"human-like\" response would be something along the lines of $y=[\\text{Cool}, \\text{which}, \\text{framework}, \\text{did}, \\text{she}, \\text{use}, \\text{?}]$.\n",
    "\n",
    "Now, let's permute $x$:\n",
    "* $x'=[\\text{The}, \\text{agent}, \\text{deployed}, \\text{the}, \\text{developer}, \\text{to}, \\text{production}]$\n",
    "\n",
    "A natural response would be $y'=[\\text{What}, \\text{the}, \\text{heck}, \\text{?}, \\text{Did}, \\text{AI}, \\text{already}, \\text{take}, \\text{over}, \\text{?}]$. \n",
    "\n",
    "If a human was permutation equivariant, we'd expect their response to be permuted in the same was the input is permuted. This means $f(x')$ would have to be $[\\text{Cool}, \\text{she}, \\text{framework}, \\text{did}, \\text{which}, \\text{use}, \\text{?}]$. \n",
    "\n",
    "This is clearly not what we want. If the input is permuted, we want a response that captures the semantical difference not merely the permutation.\n",
    "\n",
    "## Permutation Matrices\n",
    "\n",
    "A **permutation matrix** $P$ is a square matrix with exactly one 1 in each row and column (all other entries 0). Left-multiplying $P \\cdot X$ reorders the rows of $X$ according to where those 1s sit.\n",
    "\n",
    "The identity matrix $I$ is the special case where every 1 sits on the diagonal \u2014 each row maps to itself, so nothing changes. Moving a 1 off the diagonal causes the corresponding rows to swap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity I  (no change)                            Permutation P  (rows 2 & 5 swap)\n",
      "          col 0col 1col 2col 3col 4col 5col 6                col 0col 1col 2col 3col 4col 5col 6\n",
      "  row 0:  [  1   0   0   0   0   0   0 ]             row 0:  [  1   0   0   0   0   0   0 ]\n",
      "  row 1:  [  0   1   0   0   0   0   0 ]             row 1:  [  0   0   0   0   1   0   0 ]  <- swapped\n",
      "  row 2:  [  0   0   1   0   0   0   0 ]             row 2:  [  0   0   1   0   0   0   0 ]\n",
      "  row 3:  [  0   0   0   1   0   0   0 ]             row 3:  [  0   0   0   1   0   0   0 ]\n",
      "  row 4:  [  0   0   0   0   1   0   0 ]             row 4:  [  0   1   0   0   0   0   0 ]  <- swapped\n",
      "  row 5:  [  0   0   0   0   0   1   0 ]             row 5:  [  0   0   0   0   0   1   0 ]\n",
      "  row 6:  [  0   0   0   0   0   0   1 ]             row 6:  [  0   0   0   0   0   0   1 ]\n"
     ]
    }
   ],
   "source": [
    "N = 7\n",
    "\n",
    "def mat_lines(arr, title, col_prefix=\"col\", val_fmt=\"{:2d}\", swapped=set()):\n",
    "    col_w = len(val_fmt.format(0)) + 2\n",
    "    header = \"          \" + \"\".join(f\"{col_prefix} {j}\".rjust(col_w) for j in range(arr.shape[1]))\n",
    "    lines = [title, header]\n",
    "    for i, row in enumerate(arr):\n",
    "        vals = \"  \".join(val_fmt.format(v) for v in row)\n",
    "        tag = \"  <- swapped\" if i in swapped else \"\"\n",
    "        lines.append(f\"  row {i}:  [ {vals} ]{tag}\")\n",
    "    return lines\n",
    "\n",
    "def print_side_by_side(left, right, gap=6):\n",
    "    width = max(len(l) for l in left)\n",
    "    for l, r in zip(left, right):\n",
    "        print(l.ljust(width + gap) + r)\n",
    "\n",
    "I = np.eye(N, dtype=int)\n",
    "P = np.eye(N, dtype=int)\n",
    "P[[1, 4]] = P[[4, 1]]  # indices 1 and 4 \u2192 rows 2 and 5\n",
    "\n",
    "left  = mat_lines(I, \"Identity I  (no change)\")\n",
    "right = mat_lines(P, \"Permutation P  (rows 2 & 5 swap)\", swapped={1, 4})\n",
    "\n",
    "print_side_by_side(left, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Attention Permutation Equivariant?\n",
    "\n",
    "A function $f$ is permutation equivariant if for any permutation matrix $P$:\n",
    "\n",
    "$$f(PX) = P \\cdot f(X)$$\n",
    "\n",
    "Shuffle the input rows, and the output rows are shuffled in exactly the same way. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(X):\n",
    "    W_q = rng.random(size=(d_model, d_head))\n",
    "    W_k = rng.random(size=(d_model, d_head))\n",
    "    W_v = rng.random(size=(d_model, d_head))\n",
    "\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "\n",
    "    S = (Q @ K.T) / np.sqrt(d_head)\n",
    "    exp_S = np.exp(S - np.max(S, axis=1, keepdims=True))\n",
    "    P = exp_S / np.sum(exp_S, axis=1, keepdims=True)\n",
    "    \n",
    "    return P @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "N, d_model, d_head = 7, 4, 4\n\nx = np.random.rand(N, d_model)\n\ny   = attention(x)      # original\ny_p = attention(P @ x)  # permuted input\n\ndef pprint(arr, title, highlight={1, 4}):\n    header = \"          \" + \"\".join(f\"   dim {j}\" for j in range(arr.shape[1]))\n    print(title)\n    print(header)\n    print(\"    ...\")\n    for i in sorted(highlight):\n        vals = \"  \".join(f\"{v:6.3f}\" for v in arr[i])\n        print(f\"  row {i}:  [ {vals} ]  <- swapped\")\n        print(\"    ...\")\n    print()\n\npprint(y,   \"Attention(x)  \u2014 original:\")\npprint(y_p, \"Attention(Px) \u2014 permuted input:\")\n\nprint(f\"Attention(Px) == P \u00b7 Attention(x): {np.allclose(y_p, P @ y)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention **is** permutation equivariant. Shuffle the tokens in, get the same representations back \u2014 just reordered.\n",
    "\n",
    "But recall from the opening: this is exactly the property we *don't* want. Rearranging *\"The developer deployed the agent\"* into *\"The agent deployed the developer\"* should produce a fundamentally different response, not a permuted version of the same one.\n",
    "\n",
    "# Positional Encodings\n",
    "\n",
    "This is why transformers need **positional encodings** \u2014 a way to stamp each token with its position *before* attention runs, so that order is no longer invisible.\n",
    "\n",
    "In other words, we need something akin to\n",
    "\n",
    "* $x_{\\text{pos-encoded}}=[\\text{The}_1, \\text{developer}_2, \\text{deployed}_3, \\text{the}_4, \\text{agent}_5, \\text{to}_6, \\text{production}\n",
    "_7]$\n",
    "\n",
    "The original permutation would then look like:\n",
    "\n",
    "* $x'_{\\text{pos-encoded}}=[\\text{The}_1, \\text{agent}_2, \\text{deployed}_3, \\text{the}_4, \\text{developer}_2, \\text{to}_6, \\text{production}\n",
    "_7]$\n",
    "\n",
    "We enrich the word with information about its position. This way the same word will have a different representation depending on its location in the sequence ($\\text{agent}_5 \\neq \\text{agent}_2$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Need from a Positional Encoding\n",
    "\n",
    "We need a function $PE(\\text{pos})$ that maps each position to a $d$-dimensional vector, with these properties:\n",
    "\n",
    "| Property | Why |\n",
    "|----------|-----|\n",
    "| **Unique** per position | So the model can distinguish position 5 from position 50 |\n",
    "| **Compatible** with embeddings | So it doesn't dominate the token embedding or cause instability |\n",
    "| **Smooth distance** | Nearby positions should have similar encodings |\n",
    "| **Generalizes** to unseen lengths | A model trained on length 512 should handle length 1024 |\n",
    "\n",
    "\n",
    "### The shortcomings of naive approaches\n",
    "Let's investigate why a simple absolute positional encoding ($PE(\\text{pos}) = \\text{pos}$) breaks some of the above properties.\n",
    "\n",
    "Let's assume our token embeddings live in a roughly normalized space. Values of a similar order of magnitude across all tokens. For this example, we assume they are roughly between $0$ and $1$. If we added their absolute position to each dimension of the token's embedding, positional similarity (i.e., distance) loses its meaning and we would not be able to generalize to unseen sequence lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(385.20409993451403)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "N, model_dim = 100, 512\n\nembeds = rng.random(size=(N, model_dim))\nembeds += np.arange(N).reshape(N, 1)  # add absolute position to each dim\n\n# Dot-product similarity between pairs that are 4 apart\nearly = embeds[0] @ embeds[4]    # tokens 1 and 5\nlate  = embeds[95] @ embeds[99]  # tokens 96 and 100\n\nprint(f\"sim(token  1, token  5):  {early:.1f}\")\nprint(f\"sim(token 96, token 100): {late:.1f}\")\n\n# Plot similarity for all \"gap-4\" pairs\nsims = [embeds[i] @ embeds[i + 4] for i in range(N - 4)]\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(sims)\nax.set_xlabel(\"Token position $i$\")\nax.set_ylabel(\"Dot-product similarity\")\nax.set_title(\"sim(token $i$, token $i+4$) \u2014 same gap, very different similarity\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(100)[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n",
       "        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
       "        48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
       "        64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
       "        80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95,\n",
       "        96, 97, 98, 99]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, N).reshape(N, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '\u2014' (U+2014) (176109703.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mA naive approach \u2014 just use the integer position as a feature ($PE(\\text{pos}) = \\text{pos}$) \u2014 fails immediately: values grow unboundedly, and there's no meaningful relationship between dimensions.\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '\u2014' (U+2014)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A naive approach \u2014 just use the integer position as a feature ($PE(\\text{pos}) = \\text{pos}$) \u2014 fails immediately: values grow unboundedly, and there's no meaningful relationship between dimensions.\n",
    "\n",
    "What if we normalized to [0, 1]? Then $PE(\\text{pos}) = \\text{pos} / L$ where $L$ is the sequence length. But now the encoding depends on $L$, so position 5 means something different in a 10-token sequence vs. a 1000-token sequence.\n",
    "\n",
    "We need something smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Intuition: Binary Counting\n",
    "\n",
    "Before jumping to the formula, let's look at a familiar system that encodes integers: **binary representation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pos \u2502 bit3  bit2  bit1  bit0\")\n",
    "print(\"\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\")\n",
    "for pos in range(16):\n",
    "    bits = [(pos >> b) & 1 for b in range(3, -1, -1)]\n",
    "    print(f\" {pos:2d} \u2502  {'     '.join(str(b) for b in bits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the pattern:\n",
    "- **bit0** (rightmost) flips every step \u2014 frequency = 1\n",
    "- **bit1** flips every 2 steps \u2014 frequency = 1/2\n",
    "- **bit2** flips every 4 steps \u2014 frequency = 1/4\n",
    "- **bit3** flips every 8 steps \u2014 frequency = 1/8\n",
    "\n",
    "Each bit oscillates at a different frequency, and together they uniquely identify every position. **This is a positional encoding!** Each \"dimension\" (bit) captures position information at a different resolution \u2014 fast-changing bits give fine-grained position, slow-changing bits give coarse position.\n",
    "\n",
    "But binary has a problem for neural networks: it's **discrete** (0 or 1) and the transitions are sharp. We want smooth, continuous values that are friendly to gradient-based learning.\n",
    "\n",
    "The fix: replace square waves with **sinusoids**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sinusoidal Positional Encoding\n",
    "\n",
    "The original transformer paper (*Attention Is All You Need*, Vaswani et al. 2017) proposes:\n",
    "\n",
    "$$PE(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "$$PE(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{pos}$ is the token position (0, 1, 2, ...)\n",
    "- $i$ is the dimension index (0, 1, ..., $d/2 - 1$)\n",
    "- $d$ is the encoding dimension (= $d_{\\text{model}}$)\n",
    "\n",
    "Each pair of dimensions $(2i, 2i+1)$ forms a sin/cos pair at a specific frequency. The frequencies decrease geometrically from dimension 0 to dimension $d-1$:\n",
    "\n",
    "| Dimensions | Wavelength | What it captures |\n",
    "|-----------|------------|------------------|\n",
    "| 0, 1 | $2\\pi \\approx 6.3$ positions | Fine-grained: distinguishes adjacent tokens |\n",
    "| middle | ~hundreds of positions | Medium-scale patterns |\n",
    "| $d-2$, $d-1$ | $2\\pi \\cdot 10000 \\approx 62{,}832$ positions | Coarse: \"beginning\" vs \"end\" of document |\n",
    "\n",
    "Just like binary counting \u2014 but smooth and continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_pe(max_len, d_model):\n",
    "    \"\"\"Generate sinusoidal positional encoding matrix.\n",
    "    \n",
    "    Returns: (max_len, d_model) array where each row is the encoding for that position.\n",
    "    \"\"\"\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    pos = np.arange(max_len)[:, np.newaxis]          # (max_len, 1)\n",
    "    i = np.arange(0, d_model, 2)[np.newaxis, :]      # (1, d_model/2)\n",
    "    \n",
    "    # Frequencies decrease geometrically: 1, 1/10000^(2/d), 1/10000^(4/d), ...\n",
    "    angles = pos / np.power(10000, i / d_model)       # (max_len, d_model/2)\n",
    "    \n",
    "    pe[:, 0::2] = np.sin(angles)  # Even dimensions\n",
    "    pe[:, 1::2] = np.cos(angles)  # Odd dimensions\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Generate for a small example\n",
    "pe = sinusoidal_pe(max_len=100, d_model=64)\n",
    "print(f\"Shape: {pe.shape}  (100 positions, 64 dimensions)\")\n",
    "print(f\"Value range: [{pe.min():.1f}, {pe.max():.1f}]\")\n",
    "print()\n",
    "print(\"Position 0 (first 8 dims):\")\n",
    "print(pe[0, :8])\n",
    "print()\n",
    "print(\"Position 1 (first 8 dims):\")\n",
    "print(pe[1, :8])\n",
    "print()\n",
    "print(\"Position 99 (first 8 dims):\")\n",
    "print(pe[99, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"dark\")\n",
    "\n",
    "pe = sinusoidal_pe(max_len=128, d_model=64)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: heatmap of the full PE matrix\n",
    "im = axes[0].imshow(pe, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Sinusoidal Positional Encoding')\n",
    "plt.colorbar(im, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Right: individual sinusoids at different dimensions\n",
    "positions = np.arange(128)\n",
    "for dim, label in [(0, 'dim 0 (sin)'), (4, 'dim 4 (sin)'), (16, 'dim 16 (sin)'), (62, 'dim 62 (sin)')]:\n",
    "    axes[1].plot(positions, pe[:, dim], label=label, alpha=0.8)\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Different Dimensions = Different Frequencies')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].set_ylim(-1.3, 1.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows the key structure:\n",
    "- **Left columns** (low dimensions): high-frequency oscillations that change rapidly with position\n",
    "- **Right columns** (high dimensions): low-frequency oscillations that change slowly\n",
    "- Each row is a unique \"fingerprint\" for that position\n",
    "\n",
    "This is exactly the binary counting pattern \u2014 but with smooth sinusoids instead of square waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Properties\n",
    "\n",
    "### 5.1 Unique Encodings\n",
    "\n",
    "Every position gets a distinct vector. We can verify this by checking that no two positions have the same encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = sinusoidal_pe(max_len=1000, d_model=64)\n",
    "\n",
    "# Compute all pairwise distances\n",
    "# ||pe[i] - pe[j]||^2 = ||pe[i]||^2 + ||pe[j]||^2 - 2 * pe[i] . pe[j]\n",
    "norms_sq = np.sum(pe ** 2, axis=1)\n",
    "dists_sq = norms_sq[:, None] + norms_sq[None, :] - 2 * pe @ pe.T\n",
    "dists = np.sqrt(np.maximum(0, dists_sq))  # Clamp to avoid float rounding issues\n",
    "\n",
    "# Zero out the diagonal\n",
    "np.fill_diagonal(dists, np.inf)\n",
    "\n",
    "print(f\"Minimum distance between any two positions: {dists.min():.4f}\")\n",
    "print(f\"This occurs between positions {np.unravel_index(dists.argmin(), dists.shape)}\")\n",
    "print()\n",
    "print(\"Every position is distinct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Distance Structure\n",
    "\n",
    "Nearby positions should have similar encodings. Let's visualize the dot product between all pairs of position encodings \u2014 this reveals how the encoding captures distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = sinusoidal_pe(max_len=128, d_model=64)\n",
    "\n",
    "# Dot product similarity between positions\n",
    "similarity = pe @ pe.T\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: full similarity matrix\n",
    "im = axes[0].imshow(similarity, cmap='RdBu_r')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Dot Product Similarity Between Positions')\n",
    "plt.colorbar(im, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Right: similarity of position 0 with all other positions\n",
    "axes[1].plot(similarity[0], label='sim(pos 0, pos j)', alpha=0.8)\n",
    "axes[1].plot(similarity[32], label='sim(pos 32, pos j)', alpha=0.8)\n",
    "axes[1].plot(similarity[64], label='sim(pos 64, pos j)', alpha=0.8)\n",
    "axes[1].set_xlabel('Position j')\n",
    "axes[1].set_ylabel('Dot Product')\n",
    "axes[1].set_title('Similarity Decays with Distance')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix shows:\n",
    "- **Strong diagonal**: each position is most similar to itself\n",
    "- **Decay with distance**: similarity drops as positions get farther apart\n",
    "- **Symmetry**: $\\text{sim}(i, j) = \\text{sim}(j, i)$ \u2014 the dot product is symmetric\n",
    "\n",
    "This is exactly what we want \u2014 the encoding naturally captures \"closeness\" between positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Relative Position via Linear Transformation\n",
    "\n",
    "The most elegant property: for any fixed offset $k$, there exists a linear transformation $T_k$ such that:\n",
    "\n",
    "$$PE(\\text{pos} + k) = T_k \\cdot PE(\\text{pos})$$\n",
    "\n",
    "This means the model can learn to attend to relative positions using simple linear operations.\n",
    "\n",
    "**Why?** Each sin/cos pair at frequency $\\omega_i = 1/10000^{2i/d}$ behaves like a 2D rotation. Using the angle addition identities:\n",
    "\n",
    "$$\\begin{bmatrix} \\sin(\\omega_i(\\text{pos}+k)) \\\\ \\cos(\\omega_i(\\text{pos}+k)) \\end{bmatrix} = \\begin{bmatrix} \\cos(\\omega_i k) & \\sin(\\omega_i k) \\\\ -\\sin(\\omega_i k) & \\cos(\\omega_i k) \\end{bmatrix} \\begin{bmatrix} \\sin(\\omega_i \\cdot \\text{pos}) \\\\ \\cos(\\omega_i \\cdot \\text{pos}) \\end{bmatrix}$$\n",
    "\n",
    "The full $T_k$ is a block-diagonal matrix of $2 \\times 2$ rotation matrices \u2014 one per frequency. Crucially, $T_k$ depends only on the offset $k$, not on the absolute position.\n",
    "\n",
    "Let's verify this numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "pe = sinusoidal_pe(max_len=200, d_model=d_model)\n",
    "\n",
    "def build_rotation_matrix(k, d_model):\n",
    "    \"\"\"Build the block-diagonal rotation matrix T_k.\"\"\"\n",
    "    T = np.zeros((d_model, d_model))\n",
    "    for i in range(0, d_model, 2):\n",
    "        omega = 1.0 / np.power(10000, i / d_model)\n",
    "        cos_k = np.cos(omega * k)\n",
    "        sin_k = np.sin(omega * k)\n",
    "        # 2x2 rotation block for dimensions (i, i+1)\n",
    "        T[i, i] = cos_k\n",
    "        T[i, i+1] = sin_k\n",
    "        T[i+1, i] = -sin_k\n",
    "        T[i+1, i+1] = cos_k\n",
    "    return T\n",
    "\n",
    "# Test: PE(pos + k) should equal T_k @ PE(pos)\n",
    "k = 7  # Offset of 7 positions\n",
    "T_k = build_rotation_matrix(k, d_model)\n",
    "\n",
    "pos = 42  # Arbitrary starting position\n",
    "pe_direct = pe[pos + k]          # Compute PE(pos + k) directly\n",
    "pe_rotated = T_k @ pe[pos]       # Compute T_k @ PE(pos)\n",
    "\n",
    "print(f\"PE(pos={pos+k}) directly (first 8 dims):\")\n",
    "print(pe_direct[:8])\n",
    "print()\n",
    "print(f\"T_{k} @ PE(pos={pos}) (first 8 dims):\")\n",
    "print(pe_rotated[:8])\n",
    "print()\n",
    "print(f\"Max difference: {np.max(np.abs(pe_direct - pe_rotated)):.2e}\")\n",
    "print()\n",
    "\n",
    "# This works for ANY starting position\n",
    "errors = []\n",
    "for p in range(100):\n",
    "    errors.append(np.max(np.abs(pe[p + k] - T_k @ pe[p])))\n",
    "print(f\"Works for all positions: max error across pos 0-99 = {max(errors):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is powerful: the same matrix $T_k$ transforms **any** position's encoding to the encoding $k$ steps ahead. A single learned linear layer could, in principle, learn to compute \"what's 3 positions to the left?\" regardless of absolute position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How It's Used in Practice\n",
    "\n",
    "Positional encoding is **added** to the token embeddings before they enter the transformer:\n",
    "\n",
    "$$\\text{Input} = \\text{TokenEmbedding}(x) + PE$$\n",
    "\n",
    "Both have the same shape \u2014 $(\\text{seq\\_len}, d_{\\text{model}})$ \u2014 so the addition is elementwise."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Token IDs:    [  \"The\"    \"cat\"    \"sat\"    \"on\"   \"the\"    \"mat\"  ]\n",
    "                      \u2502         \u2502        \u2502        \u2502       \u2502        \u2502\n",
    "                      \u25bc         \u25bc        \u25bc        \u25bc       \u25bc        \u25bc\n",
    "    Embedding:    [ e(The)   e(cat)   e(sat)   e(on)  e(the)   e(mat) ]   \u2190 (6, d_model)\n",
    "                      +         +        +        +       +        +\n",
    "    Pos Encoding: [ PE(0)    PE(1)    PE(2)    PE(3)   PE(4)    PE(5)  ]   \u2190 (6, d_model)\n",
    "                      \u2502         \u2502        \u2502        \u2502       \u2502        \u2502\n",
    "                      \u25bc         \u25bc        \u25bc        \u25bc       \u25bc        \u25bc\n",
    "    To Attention: [ e+PE(0)  e+PE(1)  e+PE(2)  e+PE(3) e+PE(4)  e+PE(5)]  \u2190 (6, d_model)\n",
    "\n",
    "    Now \"The\" at position 0 has a different representation than \"the\" at position 4,\n",
    "    even though they're the same token \u2014 because PE(0) \u2260 PE(4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify that adding PE breaks the permutation equivariance\n",
    "seq_len, d_model, d_head = 6, 64, 64\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "W_Q = np.random.randn(d_model, d_head)\n",
    "W_K = np.random.randn(d_model, d_head)\n",
    "W_V = np.random.randn(d_model, d_head)\n",
    "\n",
    "pe = sinusoidal_pe(seq_len, d_model)\n",
    "\n",
    "# Original: add PE then compute attention\n",
    "output_original = attention(X + pe, W_Q, W_K, W_V)\n",
    "\n",
    "# Shuffled: shuffle tokens, add PE (for the NEW positions), compute attention\n",
    "perm = np.array([5, 3, 1, 4, 0, 2])\n",
    "inv_perm = np.argsort(perm)\n",
    "X_shuffled = X[perm]\n",
    "output_shuffled = attention(X_shuffled + pe, W_Q, W_K, W_V)\n",
    "\n",
    "# Un-shuffle and compare\n",
    "output_unshuffled = output_shuffled[inv_perm]\n",
    "\n",
    "diff = np.max(np.abs(output_original - output_unshuffled))\n",
    "print(f\"Without PE: outputs are identical after un-shuffling (as shown above)\")\n",
    "print(f\"With PE:    max difference after un-shuffling = {diff:.4f}\")\n",
    "print()\n",
    "print(\"The outputs are now DIFFERENT \u2014 attention is position-aware!\")\n",
    "print()\n",
    "print(\"Why? Because 'cat' at position 1 gets PE(1), but after shuffling\")\n",
    "print(\"'cat' lands at position 2 and gets PE(2). Different position \u2192\")\n",
    "print(\"different encoding \u2192 different attention pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation notes:\n",
    "- PE is computed **once** at initialization and cached \u2014 it's just a lookup table, no runtime cost\n",
    "- The encoding dimension matches $d_{\\text{model}}$ so it can be directly added\n",
    "- In the original transformer, a small dropout is applied after the addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limitations and What's Next\n",
    "\n",
    "Sinusoidal positional encoding works \u2014 it was used in the original transformer and many subsequent models. But it has a fundamental limitation: it encodes **absolute** position.\n",
    "\n",
    "Each token gets a fixed vector based on where it sits in the sequence. The model must then *learn* to extract relative position information from the difference of absolute encodings. This works, but it's indirect.\n",
    "\n",
    "**What if we could encode relative position directly into the attention computation?** Instead of modifying the input embeddings, what if the attention scores themselves were aware of the distance between query and key positions?\n",
    "\n",
    "That's the idea behind **Rotary Position Embedding (RoPE)**, which we'll explore in Part 2. RoPE rotates the query and key vectors based on their positions, so that the dot product $q_i \\cdot k_j$ naturally depends on the offset $i - j$. This makes relative position a first-class citizen of the attention mechanism.\n",
    "\n",
    "**Continue to [RoPE](./rope.ipynb) (coming soon) to see how!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}