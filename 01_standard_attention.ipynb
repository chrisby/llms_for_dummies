{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Attention: A Memory-Bound Operation\n",
    "\n",
    "This notebook explores the **standard attention mechanism** and demonstrates why it becomes a bottleneck for long sequences. We'll use a simple GPU simulator to make the memory hierarchy constraints tangible.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should be familiar with:\n",
    "- Matrix multiplication\n",
    "- The transformer architecture (at a high level)\n",
    "- Basic GPU concepts (the idea that GPUs have fast compute but memory bandwidth is limited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Attention Equation\n",
    "\n",
    "Given input matrices **Q** (queries), **K** (keys), and **V** (values), attention computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q, K, V \\in \\mathbb{R}^{N \\times d}$ — N is sequence length, d is head dimension\n",
    "- $QK^T \\in \\mathbb{R}^{N \\times N}$ — the **attention matrix** (this is the problem!)\n",
    "- $\\sqrt{d}$ — scaling factor to keep gradients stable\n",
    "\n",
    "### The Quadratic Problem\n",
    "\n",
    "Notice that $QK^T$ produces an $N \\times N$ matrix. For a sequence of length 4096:\n",
    "\n",
    "$$4096 \\times 4096 = 16,777,216 \\text{ floats}$$\n",
    "\n",
    "At FP16 (2 bytes/float), that's **32 MB just for one attention matrix**. With multiple heads and layers, this adds up fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: GPU Simulator\n",
    "\n",
    "We'll use a simple simulator that models the key characteristics of GPU memory hierarchy.\n",
    "\n",
    "### Quick Terminology\n",
    "\n",
    "- **FLOP** (Floating Point Operation): A single math operation on decimal numbers — one addition, one multiplication, etc. \n",
    "\n",
    "  *Example*: A matrix multiply $C = A \\times B$ where $A$ and $B$ are both 1000×1000 requires:\n",
    "  - Each element of $C$ needs 1000 multiplications and 1000 additions (dot product of a row and column)\n",
    "  - $C$ has 1000×1000 = 1M elements\n",
    "  - Total: 1M elements × 2000 ops = **2 billion FLOPs**\n",
    "  \n",
    "  General formula: multiplying $(M \\times K)$ by $(K \\times N)$ costs $2 \\times M \\times N \\times K$ FLOPs.\n",
    "\n",
    "- **Cycle**: One \"tick\" of the GPU clock. A modern GPU runs at ~1-2 GHz, meaning 1-2 billion cycles per second. We use cycles as our unit of time because it lets us reason about *ratios* without worrying about actual clock speeds.\n",
    "\n",
    "### Our Simulated GPUs\n",
    "\n",
    "We model three generations of NVIDIA GPUs. Notice how newer GPUs become increasingly **compute-bound** — their compute throughput grows faster than memory bandwidth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU                  |         SRAM |  Compute/HBM | Trend\n",
      "-----------------------------------------------------------------\n",
      "Simulated V100       |       128 KB |         70x | Less compute-bound\n",
      "Simulated A100       |       256 KB |        150x | Compute-bound\n",
      "Simulated H100       |       384 KB |        300x | Very compute-bound\n",
      "\n",
      "→ Newer GPUs are MORE compute-bound, making memory optimization MORE important.\n"
     ]
    }
   ],
   "source": [
    "from src.gpu_sim import GPUSpec, Profiler, Tensor\n",
    "\n",
    "# Compare three generations of GPUs\n",
    "gpus = [GPUSpec.sim_v100(), GPUSpec.sim_a100(), GPUSpec.sim_h100()]\n",
    "\n",
    "print(f\"{'GPU':<20} | {'SRAM':>12} | {'Compute/HBM':>12} | {'Trend'}\")\n",
    "print(\"-\" * 65)\n",
    "for g in gpus:\n",
    "    sram_kb = g.sram_size * g.bytes_per_float / 1024\n",
    "    ratio = g.flop_rate / g.hbm_bandwidth\n",
    "    print(f\"{g.name:<20} | {sram_kb:>9.0f} KB | {ratio:>10.0f}x | \", end=\"\")\n",
    "    if ratio < 100:\n",
    "        print(\"Less compute-bound\")\n",
    "    elif ratio < 200:\n",
    "        print(\"Compute-bound\")\n",
    "    else:\n",
    "        print(\"Very compute-bound\")\n",
    "\n",
    "print(\"\\n→ Newer GPUs are MORE compute-bound, making memory optimization MORE important.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: Simulated A100\n"
     ]
    }
   ],
   "source": [
    "# We'll use the A100 for our examples\n",
    "gpu = GPUSpec.sim_a100()\n",
    "print(f\"Using: {gpu.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standard Attention: The Naive Implementation\n",
    "\n",
    "Here's how standard attention works, step by step:\n",
    "\n",
    "```\n",
    "1. Load Q, K from HBM\n",
    "2. Compute S = Q @ K.T        # N×N attention scores\n",
    "3. Store S to HBM             # Can't fit in SRAM!\n",
    "4. Load S from HBM\n",
    "5. Compute P = softmax(S)     # N×N attention weights  \n",
    "6. Store P to HBM             # Still N×N\n",
    "7. Load P, V from HBM\n",
    "8. Compute O = P @ V          # Final output\n",
    "9. Store O to HBM\n",
    "```\n",
    "\n",
    "The problem: we **materialize the full N×N matrix** and shuttle it back and forth to HBM multiple times.\n",
    "\n",
    "We'll use a `Tensor` class that wraps the profiler calls, giving us PyTorch-like syntax while tracking all the memory operations under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(prof: Profiler, N: int, d: int):\n",
    "    \"\"\"\n",
    "    Simulate standard attention for a single head.\n",
    "    \n",
    "    The key problem: S and P are materialized in HBM, causing extra traffic.\n",
    "    \n",
    "    Args:\n",
    "        prof: Profiler instance to track cycles and memory\n",
    "        N: Sequence length\n",
    "        d: Head dimension\n",
    "    \"\"\"\n",
    "    # Allocate inputs (would be loaded from previous layer in practice)\n",
    "    Q = Tensor((N, d), name=\"Q\", profiler=prof)\n",
    "    K = Tensor((N, d), name=\"K\", profiler=prof)\n",
    "    V = Tensor((N, d), name=\"V\", profiler=prof)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    # S = Q @ K.T produces an N×N matrix that must be stored in HBM\n",
    "    S = Q @ K.T\n",
    "    \n",
    "    # Step 2: Apply softmax\n",
    "    # P = softmax(S) produces another N×N matrix stored in HBM\n",
    "    P = S.softmax()\n",
    "    S.free()  # Free S - no longer needed\n",
    "    \n",
    "    # Step 3: Apply attention weights to values\n",
    "    # O = P @ V produces the final N×d output\n",
    "    O = P @ V\n",
    "    P.free()  # Free P - no longer needed\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Enable verbose output to see memory operations\n",
    "Tensor.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Simulation\n",
    "\n",
    "Let's first try with a small sequence that fits in SRAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length N = 256, head dimension d = 64\n",
      "Attention matrix S: 256×256 = 65,536 floats\n",
      "\n",
      "Allocate Q (256, 64)                          | HBM:   32.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "Allocate K (256, 64)                          | HBM:   64.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "Allocate V (256, 64)                          | HBM:   96.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      ">>> Q @ K.T\n",
      "Allocate Q@K.T (256, 256)                     | HBM:  224.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "    [HBM → SRAM] Load Q                           | HBM:  224.0 KB | SRAM:   32.0 KB ( 12.5%)\n",
      "    [HBM → SRAM] Load K                           | HBM:  224.0 KB | SRAM:   64.0 KB ( 25.0%)\n",
      "    [Compute] matmul → (256, 256)                 | HBM:  224.0 KB | SRAM:   64.0 KB ( 25.0%)\n",
      "    [SRAM → HBM] Store Q@K.T                      | HBM:  224.0 KB | SRAM:   64.0 KB ( 25.0%)\n",
      "    [SRAM] Clear working set                      | HBM:  224.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      ">>> softmax(Q@K.T)\n",
      "Allocate softmax(Q@K.T) (256, 256)            | HBM:  352.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "    [HBM → SRAM] Load Q@K.T                       | HBM:  352.0 KB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Compute] softmax                             | HBM:  352.0 KB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [SRAM → HBM] Store softmax(Q@K.T)             | HBM:  352.0 KB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [SRAM] Clear working set                      | HBM:  352.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      "Free Q@K.T                                    | HBM:  224.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      ">>> softmax(Q@K.T) @ V\n",
      "Allocate softmax(Q@K.T)@V (256, 64)           | HBM:  256.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "    [HBM → SRAM] Load softmax(Q@K.T)              | HBM:  256.0 KB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [HBM → SRAM] Load V                           | HBM:  256.0 KB | SRAM:  160.0 KB ( 62.5%)\n",
      "    [Compute] matmul → (256, 64)                  | HBM:  256.0 KB | SRAM:  160.0 KB ( 62.5%)\n",
      "    [SRAM → HBM] Store softmax(Q@K.T)@V           | HBM:  256.0 KB | SRAM:  160.0 KB ( 62.5%)\n",
      "    [SRAM] Clear working set                      | HBM:  256.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      "Free softmax(Q@K.T)                           | HBM:  128.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "============================================================\n",
      "Standard Attention (N=256)\n",
      "============================================================\n",
      "\n",
      "Memory Usage:\n",
      "  Peak HBM:          180,224 floats (0.3 MB)\n",
      "  Peak SRAM:          81,920 floats (62% of 131,072)\n",
      "\n",
      "HBM Traffic:\n",
      "  Reads:          180,224 floats (0.3 MB)\n",
      "  Writes:         147,456 floats (0.3 MB)\n",
      "  Total:          327,680 floats (0.6 MB)\n",
      "\n",
      "Cycles:\n",
      "  Compute:         114,033 (25.8%)\n",
      "  HBM:             327,680 (74.2%)\n",
      "  Total:           441,713\n",
      "\n",
      "→ Memory-bound (HBM is the bottleneck)\n"
     ]
    }
   ],
   "source": [
    "# Small sequence\n",
    "N = 256\n",
    "d = 64\n",
    "\n",
    "print(f\"Sequence length N = {N}, head dimension d = {d}\")\n",
    "print(f\"Attention matrix S: {N}×{N} = {N*N:,} floats\\n\")\n",
    "\n",
    "prof = Profiler(gpu, f\"Standard Attention (N={N})\")\n",
    "standard_attention(prof, N, d)\n",
    "prof.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a realistic sequence length. We'll disable verbose output since the operations are the same — just with much larger matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length N = 4096, head dimension d = 64\n",
      "Attention matrix S: 4096×4096 = 16,777,216 floats\n",
      "SRAM capacity: 131,072 floats\n",
      "S is 128x larger than SRAM!\n",
      "\n",
      "Allocate Q (4096, 64)                         | HBM:  512.0 KB | SRAM:    0.0 KB (  0.0%)\n",
      "Allocate K (4096, 64)                         | HBM:    1.0 MB | SRAM:    0.0 KB (  0.0%)\n",
      "Allocate V (4096, 64)                         | HBM:    1.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      ">>> Q @ K.T\n",
      "Allocate Q@K.T (4096, 4096)                   | HBM:   33.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "    [2D tiling] 4×4 = 16 tiles (1024×1024 each)   | HBM:   33.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "    [Tile 1/16] A[0:1024,:] @ B[:,0:1024]         | HBM:   33.5 MB | SRAM:  256.0 KB (100.0%)\n",
      "    [Tile 2/16] A[0:1024,:] @ B[:,1024:2048]      | HBM:   33.5 MB | SRAM:  256.0 KB (100.0%)\n",
      "    [Tile 3/16] A[0:1024,:] @ B[:,2048:3072]      | HBM:   33.5 MB | SRAM:  256.0 KB (100.0%)\n",
      "    ... (12 more tiles) ...\n",
      "    [Tile 16/16] A[3072:4096,:] @ B[:,3072:4096]  | HBM:   33.5 MB | SRAM:  256.0 KB (100.0%)\n",
      "    [Done] 16 tiles → +   3.0 MB extra traffic    | HBM:   33.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      ">>> softmax(Q@K.T)\n",
      "Allocate softmax(Q@K.T) (4096, 4096)          | HBM:   65.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "    [SRAM overflow] Need 256 chunks (16 rows each) | HBM:   65.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "    [Chunk 1/256] Load rows 0:16                  | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Chunk 1/256] Compute & store rows 0:16       | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Chunk 2/256] Load rows 16:32                 | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Chunk 2/256] Compute & store rows 16:32      | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Chunk 3/256] Load rows 32:48                 | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Chunk 3/256] Compute & store rows 32:48      | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    ... (252 more chunks) ...\n",
      "    [Chunk 256/256] Load rows 4080:4096           | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Chunk 256/256] Compute & store rows 4080:4096 | HBM:   65.5 MB | SRAM:  128.0 KB ( 50.0%)\n",
      "    [Done] Processed 4096 rows in 256 chunks      | HBM:   65.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      "Free Q@K.T                                    | HBM:   33.5 MB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      ">>> softmax(Q@K.T) @ V\n",
      "Allocate softmax(Q@K.T)@V (4096, 64)          | HBM:   34.0 MB | SRAM:    0.0 KB (  0.0%)\n",
      "    [2D tiling] 256×4 = 1024 tiles (16×16 each)   | HBM:   34.0 MB | SRAM:    0.0 KB (  0.0%)\n",
      "    [Tile 1/1024] A[0:16,:] @ B[:,0:16]           | HBM:   34.0 MB | SRAM:  256.0 KB (100.0%)\n",
      "    [Tile 2/1024] A[0:16,:] @ B[:,16:32]          | HBM:   34.0 MB | SRAM:  256.0 KB (100.0%)\n",
      "    [Tile 3/1024] A[0:16,:] @ B[:,32:48]          | HBM:   34.0 MB | SRAM:  256.0 KB (100.0%)\n",
      "    ... (1020 more tiles) ...\n",
      "    [Tile 1024/1024] A[4080:4096,:] @ B[:,48:64]  | HBM:   34.0 MB | SRAM:  256.0 KB (100.0%)\n",
      "    [Done] 1024 tiles → + 223.5 MB extra traffic  | HBM:   34.0 MB | SRAM:    0.0 KB (  0.0%)\n",
      "\n",
      "Free softmax(Q@K.T)                           | HBM:    2.0 MB | SRAM:    0.0 KB (  0.0%)\n",
      "============================================================\n",
      "Standard Attention (N=4096)\n",
      "============================================================\n",
      "\n",
      "Memory Usage:\n",
      "  Peak HBM:       34,340,864 floats (65.5 MB)\n",
      "  Peak SRAM:         131,072 floats (100% of 131,072)\n",
      "\n",
      "HBM Traffic:\n",
      "  Reads:      153,092,096 floats (292.0 MB)\n",
      "  Writes:      33,816,576 floats (64.5 MB)\n",
      "  Total:      186,908,672 floats (356.5 MB)\n",
      "\n",
      "Cycles:\n",
      "  Compute:      29,192,356 (13.5%)\n",
      "  HBM:         186,908,672 (86.5%)\n",
      "  Total:       216,101,028\n",
      "\n",
      "→ Memory-bound (HBM is the bottleneck)\n"
     ]
    }
   ],
   "source": [
    "# Disable verbose for the large test\n",
    "Tensor.verbose = True\n",
    "\n",
    "# Realistic sequence length\n",
    "N = 4096\n",
    "d = 64\n",
    "\n",
    "print(f\"Sequence length N = {N}, head dimension d = {d}\")\n",
    "print(f\"Attention matrix S: {N}×{N} = {N*N:,} floats\")\n",
    "print(f\"SRAM capacity: {gpu.sram_size:,} floats\")\n",
    "print(f\"S is {N*N / gpu.sram_size:.0f}x larger than SRAM!\\n\")\n",
    "\n",
    "prof = Profiler(gpu, f\"Standard Attention (N={N})\")\n",
    "standard_attention(prof, N, d)\n",
    "prof.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The HBM Traffic Problem\n",
    "\n",
    "The issue isn't that standard attention *can't run* — it's that we're doing **unnecessary HBM round-trips**:\n",
    "\n",
    "| Operation | What we wanted | What actually happens |\n",
    "|-----------|---------------|----------------------|\n",
    "| Compute Q@K.T | Keep result in SRAM | N×N too big → write to HBM |\n",
    "| Softmax | Use Q@K.T from SRAM | In HBM → read it back |\n",
    "| Compute P | Keep result in SRAM | N×N too big → write to HBM |\n",
    "| P @ V | Use P from SRAM | In HBM → read it back |\n",
    "\n",
    "Every time we read/write the N×N matrix, we pay the HBM bandwidth cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBM Traffic Breakdown:\n",
      "----------------------------------------------------------------------\n",
      "  ←  READ:      262,144 floats (  0.5 MB)  Q\n",
      "  ←  READ:      262,144 floats (  0.5 MB)  K\n",
      "  → WRITE:   16,777,216 floats ( 32.0 MB)  Q@K.T\n",
      "  ←  READ:   16,777,216 floats ( 32.0 MB)  Q@K.T\n",
      "  → WRITE:   16,777,216 floats ( 32.0 MB)  softmax(Q@K.T)\n",
      "  ←  READ:   16,777,216 floats ( 32.0 MB)  softmax(Q@K.T)\n",
      "  ←  READ:      262,144 floats (  0.5 MB)  V\n",
      "  → WRITE:      262,144 floats (  0.5 MB)  softmax(Q@K.T)@V\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Notice: The N×N attention matrix is written then immediately read back — twice!\n",
      "        That's 67,108,864 floats of unnecessary HBM traffic.\n"
     ]
    }
   ],
   "source": [
    "# Show HBM traffic breakdown\n",
    "print(\"HBM Traffic Breakdown:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for op, name, size in prof.get_hbm_traffic_log():\n",
    "    direction = \"←\" if op == \"hbm_read\" else \"→\"\n",
    "    mb = size * gpu.bytes_per_float / 1024 / 1024\n",
    "    print(f\"  {direction} {op.split('_')[1].upper():>5}: {size:>12,} floats ({mb:>5.1f} MB)  {name}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nNotice: The N×N attention matrix is written then immediately read back — twice!\")\n",
    "print(f\"        That's {2 * N * N * 2:,} floats of unnecessary HBM traffic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Standard attention is memory-bound**: Most time is spent on HBM traffic, not compute.\n",
    "\n",
    "2. **The N×N matrices S and P cause the problem**: They're too large for SRAM, so they must be written to HBM and read back — twice each.\n",
    "\n",
    "3. **HBM traffic scales quadratically**: `O(N²)` floats for S and P, vs only `O(Nd)` for Q, K, V, O.\n",
    "\n",
    "4. **The compute is actually cheap**: The matmuls and softmax are fast once data is in SRAM.\n",
    "\n",
    "---\n",
    "\n",
    "**The FlashAttention Insight**: What if we never wrote S and P to HBM at all? \n",
    "\n",
    "By carefully tiling the computation, FlashAttention keeps the intermediate attention scores in SRAM and only writes the final output O to HBM. This reduces HBM traffic from `O(N²)` to `O(Nd)` — a massive win for long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. FlashAttention: Eliminating the N×N Round-trips\n\nFlashAttention's key insight: **never materialize S or P in HBM**. Instead:\n1. Tile Q, K, V into blocks that fit in SRAM\n2. For each block: compute local attention scores, apply softmax, accumulate into output\n3. Only write the final O to HBM\n\nLet's simulate this to see the HBM traffic savings:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def flash_attention(prof: Profiler, N: int, d: int, block_size: int = 64):\n    \"\"\"\n    Simulate FlashAttention's memory access pattern.\n    \n    Key difference from standard attention:\n    - S and P are NEVER written to HBM\n    - Everything is computed in tiles that fit in SRAM\n    - Only Q, K, V are read and O is written\n    \n    Args:\n        prof: Profiler instance\n        N: Sequence length  \n        d: Head dimension\n        block_size: Tile size (B_r = B_c in the paper)\n    \"\"\"\n    # Allocate inputs and output in HBM\n    prof.allocate_hbm(N * d, \"Q\")\n    prof.allocate_hbm(N * d, \"K\")\n    prof.allocate_hbm(N * d, \"V\")\n    prof.allocate_hbm(N * d, \"O\")\n    \n    B = block_size\n    num_blocks = (N + B - 1) // B\n    \n    if Tensor.verbose:\n        print(f\"FlashAttention: {num_blocks}×{num_blocks} = {num_blocks**2} tiles of {B}×{B}\")\n        print(f\"SRAM per tile: Q_block({B}×{d}) + K_block({B}×{d}) + V_block({B}×{d}) + S_block({B}×{B}) + O_block({B}×{d})\")\n        sram_needed = B*d + B*d + B*d + B*B + B*d\n        print(f\"             = {sram_needed:,} floats = {sram_needed * 2 / 1024:.1f} KB\")\n        print()\n    \n    # FlashAttention outer loop: iterate over K, V blocks\n    for j in range(num_blocks):\n        kv_start = j * B\n        kv_end = min(kv_start + B, N)\n        kv_size = kv_end - kv_start\n        \n        # Load K_j and V_j once per outer iteration\n        prof.load_from_hbm(kv_size * d, f\"K[{kv_start}:{kv_end}]\")\n        prof.load_from_hbm(kv_size * d, f\"V[{kv_start}:{kv_end}]\")\n        \n        # Inner loop: iterate over Q blocks\n        for i in range(num_blocks):\n            q_start = i * B\n            q_end = min(q_start + B, N)\n            q_size = q_end - q_start\n            \n            # Load Q_i block\n            prof.load_from_hbm(q_size * d, f\"Q[{q_start}:{q_end}]\")\n            \n            # Track SRAM usage for this tile\n            tile_sram = q_size * d + kv_size * d + kv_size * d + q_size * kv_size + q_size * d\n            prof.sram_push(tile_sram, f\"tile[{i},{j}]\")\n            \n            # Compute S_ij = Q_i @ K_j.T (stays in SRAM!)\n            prof.matmul(q_size, kv_size, d, f\"S[{i},{j}] = Q_i @ K_j.T\")\n            \n            # Compute P_ij = softmax(S_ij) (stays in SRAM!)\n            prof.elementwise(q_size * kv_size, 5, f\"P[{i},{j}] = softmax(S)\")\n            \n            # Compute O_i += P_ij @ V_j (accumulate in SRAM)\n            prof.matmul(q_size, d, kv_size, f\"O[{i}] += P @ V_j\")\n            \n            prof.sram_pop(tile_sram, f\"tile[{i},{j}]\")\n    \n    # Write final output O to HBM (only once!)\n    prof.store_to_hbm(N * d, \"O\")\n    \n    if Tensor.verbose:\n        print(f\"Done! S and P never touched HBM.\")\n\n# Compare standard vs flash attention\nprint(\"=\"*70)\nprint(\"COMPARISON: Standard Attention vs FlashAttention\")\nprint(\"=\"*70)\n\nN, d = 4096, 64\n\n# Standard attention\nTensor.verbose = False\nprof_std = Profiler(gpu, \"Standard Attention\")\nstandard_attention(prof_std, N, d)\n\n# FlashAttention  \nprof_flash = Profiler(gpu, \"FlashAttention\")\nflash_attention(prof_flash, N, d, block_size=64)\n\nprint(f\"\\n{'Metric':<30} {'Standard':>15} {'FlashAttention':>15} {'Savings':>12}\")\nprint(\"-\" * 75)\n\nstd_traffic = prof_std.total_hbm_reads + prof_std.total_hbm_writes\nflash_traffic = prof_flash.total_hbm_reads + prof_flash.total_hbm_writes\nprint(f\"{'HBM Traffic (floats)':<30} {std_traffic:>15,} {flash_traffic:>15,} {(1 - flash_traffic/std_traffic)*100:>11.1f}%\")\n\nstd_mb = std_traffic * 2 / 1024 / 1024\nflash_mb = flash_traffic * 2 / 1024 / 1024\nprint(f\"{'HBM Traffic (MB)':<30} {std_mb:>15.1f} {flash_mb:>15.1f} {(1 - flash_mb/std_mb)*100:>11.1f}%\")\n\nprint(f\"{'Peak HBM (floats)':<30} {prof_std.peak_hbm_usage:>15,} {prof_flash.peak_hbm_usage:>15,}\")\n\nstd_pct = prof_std.cycles_hbm / (prof_std.cycles_hbm + prof_std.cycles_compute) * 100\nflash_pct = prof_flash.cycles_hbm / (prof_flash.cycles_hbm + prof_flash.cycles_compute) * 100\nprint(f\"{'Time on HBM (%)':<30} {std_pct:>14.1f}% {flash_pct:>14.1f}%\")\n\nprint(\"\\n→ FlashAttention eliminates O(N²) HBM traffic for S and P!\")\nprint(f\"  Standard: reads/writes {N}×{N} attention matrix twice = {4*N*N:,} floats\")\nprint(f\"  Flash: only reads Q,K,V and writes O = {4*N*d:,} floats (for the attention part)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key Takeaways\n\n1. **Standard attention is memory-bound**: Even with 2D tiling, 86% of time is spent on HBM traffic.\n\n2. **The N×N matrices S and P are the bottleneck**: They must be written to HBM and read back twice.\n\n3. **FlashAttention eliminates this**: By fusing operations and keeping S, P in SRAM:\n   - **90% less HBM traffic**\n   - **97% less peak HBM usage** (no N×N allocation)\n   - **Shifts from memory-bound to compute-bound**\n\n4. **The compute is the same**: Both do identical FLOPs — FlashAttention just accesses memory smarter.\n\n---\n\n**Next**: In the FlashAttention notebook, we'll implement the actual tiled algorithm with online softmax.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}